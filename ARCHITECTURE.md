# EasyPrompt - Comprehensive Architecture Documentation

> AI Prompt Optimization Platform with Multi-Provider Support (Commercial + Open-Source)

**Version:** 1.0.0
**Last Updated:** 2025-11-18
**Framework:** Next.js 16
**Status:** Planning Complete - Ready for Implementation

---

## Table of Contents

1. [Project Overview](#project-overview)
2. [Technology Stack](#technology-stack)
3. [Supported AI Providers](#supported-ai-providers)
4. [Architecture Design](#architecture-design)
5. [File Structure](#file-structure)
6. [Provider Implementation Details](#provider-implementation-details)
7. [Next.js 16 Features Implementation](#nextjs-16-features-implementation)
8. [Implementation Phases](#implementation-phases)
9. [Environment Configuration](#environment-configuration)
10. [Dependencies](#dependencies)
11. [Success Metrics](#success-metrics)
12. [Deployment Guide](#deployment-guide)

---

## Project Overview

### Problem Statement

Most users don't follow prompt engineering best practices when interacting with AI, resulting in suboptimal responses. Users need a tool that:
- Analyzes their prompts for issues
- Generates optimized versions automatically
- Shows before/after comparison with actual AI responses
- Educates users on prompt engineering principles

### Solution: EasyPrompt

**EasyPrompt** is a web platform that optimizes AI prompts across multiple providers (both commercial and open-source), providing:
- Real-time prompt analysis and optimization
- Multi-provider comparison
- Educational content
- Support for local/free models (privacy + zero cost)
- Production-ready Next.js 16 implementation

### Key Features

- âœ… **Multi-Provider Support** - 7 AI providers (3 commercial + 4 open-source)
- âœ… **Local Model Support** - Run Ollama locally for complete privacy
- âœ… **Provider Health Monitoring** - Real-time status checks
- âœ… **Dynamic Model Discovery** - Auto-detect installed Ollama models
- âœ… **Before/After Comparison** - See actual AI response improvements
- âœ… **Educational Focus** - Learn prompt engineering best practices
- âœ… **Rate Limiting** - Built-in protection against abuse
- âœ… **Type-Safe** - Full TypeScript implementation
- âœ… **Next.js 16** - Latest features (Turbopack, "use cache", proxy.ts, React 19.2)

---

## Technology Stack

### Core Framework

| Technology | Version | Purpose |
|------------|---------|---------|
| Next.js | 16.0+ | Full-stack React framework |
| React | 19.2 | UI library with View Transitions |
| TypeScript | 5.1.0+ | Type safety |
| Node.js | 20.9.0+ | Runtime (required for Next.js 16) |

### Build & Development

| Technology | Purpose |
|------------|---------|
| Turbopack | Default bundler in Next.js 16 (10x faster) |
| React Compiler | Automatic component memoization |
| ESLint | Code linting |
| Prettier | Code formatting |

### Styling & UI

| Technology | Purpose |
|------------|---------|
| Tailwind CSS | 4.x - Utility-first CSS |
| Shadcn/UI | Accessible UI components |
| Radix UI | Headless UI primitives |
| Lucide React | Icon library |

### AI Provider SDKs

#### Commercial Providers
- `@anthropic-ai/sdk` ^0.30.0 - Claude 3.5 Sonnet, Opus, Haiku
- `openai` ^4.70.0 - GPT-4, GPT-4 Turbo, GPT-3.5
- `@google/generative-ai` ^0.21.0 - Gemini 1.5 Pro/Flash

#### Open-Source Providers
- `ollama` ^0.6.3 - Local models (Llama, Mistral, Qwen)
- `@huggingface/inference` ^4.13.3 - 1000+ cloud models
- `replicate` ^0.34.1 - 100+ models (pay-per-use)
- OpenAI SDK (for Together AI) - OpenAI-compatible

### Infrastructure

| Technology | Purpose |
|------------|---------|
| Vercel | Deployment platform |
| Upstash Redis | Rate limiting (production) |
| In-memory cache | Rate limiting (development) |

---

## Supported AI Providers

### Provider Matrix

| Provider | Category | Location | Cost | Models | Priority | Key Benefit |
|----------|----------|----------|------|--------|----------|-------------|
| **Anthropic Claude** | Commercial | Cloud | Paid | Sonnet, Opus, Haiku | â­ P1 | Best quality |
| **OpenAI GPT** | Commercial | Cloud | Paid | GPT-4, GPT-4 Turbo, GPT-3.5 | â­ P1 | Most popular |
| **Google Gemini** | Commercial | Cloud | Paid | Gemini 1.5 Pro/Flash | â­ P1 | Fast & capable |
| **Ollama** | Open-Source | **Local** | **FREE** | Llama 3.2, Mistral, Qwen | â­ P1 | Privacy, no cost |
| **Hugging Face** | Open-Source | Cloud | Free tier | 1000+ models | â­ P1 | Wide selection |
| **Together AI** | Open-Source | Cloud | Paid | 200+ models | ğŸ”¸ P2 | Fast inference |
| **Replicate** | Open-Source | Cloud | Pay-per-use | 100+ models | ğŸ”¸ P2 | Easy deployment |
| **Cohere** | Commercial | Cloud | Paid | Command R+, R | ğŸ”¹ P3 | Enterprise |

### Provider Categories

**Commercial Providers (ğŸ¢)**
- Premium quality models
- Require API keys
- Paid per token usage
- Best for production quality

**Open-Source Providers (ğŸ”“)**
- Free or low-cost options
- Local or cloud deployment
- Privacy-focused (Ollama)
- Wide model selection

### Provider Capabilities

```typescript
interface ProviderCapabilities {
  streaming: boolean           // Server-sent events support
  functionCalling: boolean     // Tool/function calling
  vision: boolean             // Image understanding
  embeddings: boolean         // Vector embeddings
  maxTokens: number           // Maximum output tokens
}
```

---

## Architecture Design

### Provider Abstraction Pattern

**Design Pattern:** Strategy + Factory + Adapter

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ProviderFactory                       â”‚
â”‚  (Creates and manages provider instances)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                       â”‚
         â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AIProvider     â”‚    â”‚  AIProvider     â”‚
â”‚  Interface      â”‚    â”‚  Interface      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                      â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â–¼         â–¼            â–¼         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Anthropicâ”‚ â”‚OpenAI â”‚  â”‚Ollama  â”‚ â”‚Hugging â”‚
â”‚Providerâ”‚ â”‚Providerâ”‚  â”‚Providerâ”‚ â”‚  Face  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Interfaces

#### AIProvider Interface

```typescript
export interface AIProvider {
  // Metadata
  readonly metadata: ProviderMetadata
  readonly models: Model[]
  readonly defaultModel: string
  readonly capabilities: ProviderCapabilities

  // Core methods
  analyzePrompt(prompt: string, model?: string): Promise<AnalysisResult>
  optimizePrompt(prompt: string, analysis: AnalysisResult, model?: string): Promise<OptimizedPrompt>
  generatePreview(prompt: string, model?: string): Promise<string>

  // Health & Discovery
  healthCheck(): Promise<HealthStatus>
  discoverModels?(): Promise<Model[]>  // For Ollama

  // Utilities
  getModelInfo(modelId: string): Model | undefined
  isAvailable(): boolean
  estimateCost?(tokens: number, model?: string): number
}
```

#### ProviderMetadata

```typescript
export interface ProviderMetadata {
  name: ProviderType
  displayName: string
  category: 'commercial' | 'open-source'
  location: 'local' | 'cloud'
  requiresApiKey: boolean
  isOpenAICompatible: boolean
  supportsModelDiscovery: boolean
  icon: string
  documentation: string
  defaultEndpoint?: string  // For local providers
}
```

#### Model Definition

```typescript
export interface Model {
  id: string
  name: string
  tier: 'free' | 'fast' | 'standard' | 'premium'
  description?: string
  contextWindow?: number
  pricing?: {
    input: number   // per 1M tokens
    output: number  // per 1M tokens
  }
  provider: ProviderType
  openSource?: boolean
}
```

### Data Flow Architecture

```
User Input â†’ Server Action â†’ Provider Factory â†’ Specific Provider â†’ AI API
                                                                        â†“
User Interface â† Response Normalization â† Provider Response â† AI Response
```

**Step-by-step:**
1. User enters prompt in Client Component
2. Client Component calls Server Action
3. Server Action uses ProviderFactory to get provider instance
4. Provider instance calls AI API (Anthropic, OpenAI, Ollama, etc.)
5. Provider normalizes response to standard format
6. Server Action returns normalized response
7. Client Component displays results

### Caching Strategy (Next.js 16)

**Server Components with "use cache":**
- Static content (model lists, templates, guides)
- Provider metadata
- Component-level caching for optimized prompts

**No caching:**
- User inputs
- Real-time analysis
- Health checks
- Dynamic model discovery

---

## File Structure

```
easyprompt/
â”œâ”€â”€ proxy.ts                          # Rate limiting (replaces middleware.ts)
â”œâ”€â”€ next.config.ts                    # Next.js 16 configuration
â”œâ”€â”€ tsconfig.json                     # TypeScript configuration
â”œâ”€â”€ tailwind.config.ts                # Tailwind CSS configuration
â”œâ”€â”€ package.json                      # Dependencies
â”œâ”€â”€ .env.local                        # Environment variables (gitignored)
â”œâ”€â”€ .env.example                      # Environment template
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md                         # Project documentation
â”œâ”€â”€ ARCHITECTURE.md                   # This file
â”‚
â”œâ”€â”€ app/                              # Next.js App Router
â”‚   â”œâ”€â”€ layout.tsx                    # Root layout with React 19.2
â”‚   â”œâ”€â”€ page.tsx                      # Home page (main optimizer)
â”‚   â”œâ”€â”€ loading.tsx                   # Loading UI
â”‚   â”œâ”€â”€ error.tsx                     # Error boundary
â”‚   â”œâ”€â”€ globals.css                   # Global styles
â”‚   â”‚
â”‚   â”œâ”€â”€ compare/
â”‚   â”‚   â””â”€â”€ page.tsx                  # Multi-provider comparison
â”‚   â”‚
â”‚   â”œâ”€â”€ providers/
â”‚   â”‚   â””â”€â”€ page.tsx                  # Provider status & management
â”‚   â”‚
â”‚   â”œâ”€â”€ guide/
â”‚   â”‚   â””â”€â”€ page.tsx                  # Best practices guide
â”‚   â”‚
â”‚   â”œâ”€â”€ templates/
â”‚   â”‚   â””â”€â”€ page.tsx                  # Prompt templates
â”‚   â”‚
â”‚   â””â”€â”€ settings/
â”‚       â””â”€â”€ page.tsx                  # User settings (custom endpoints)
â”‚
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ server/                       # Server Components (async, "use cache")
â”‚   â”‚   â”œâ”€â”€ OptimizedPromptDisplay.tsx
â”‚   â”‚   â”œâ”€â”€ AnalysisResults.tsx
â”‚   â”‚   â”œâ”€â”€ ComparisonGrid.tsx
â”‚   â”‚   â”œâ”€â”€ ProviderBadge.tsx
â”‚   â”‚   â”œâ”€â”€ ProviderHealthStatus.tsx
â”‚   â”‚   â”œâ”€â”€ ModelList.tsx
â”‚   â”‚   â””â”€â”€ ImprovementsList.tsx
â”‚   â”‚
â”‚   â””â”€â”€ client/                       # Client Components ('use client')
â”‚       â”œâ”€â”€ PromptInput.tsx           # Text area with state
â”‚       â”œâ”€â”€ ProviderSelector.tsx      # Dropdown with categories
â”‚       â”œâ”€â”€ ModelSelector.tsx         # Model selection
â”‚       â”œâ”€â”€ ProviderHealthCheck.tsx   # Real-time health display
â”‚       â”œâ”€â”€ LocalProviderSetup.tsx    # Setup instructions
â”‚       â”œâ”€â”€ CopyButton.tsx            # Copy to clipboard
â”‚       â”œâ”€â”€ OptimizeButton.tsx        # Submit button
â”‚       â””â”€â”€ ThemeToggle.tsx           # Dark mode toggle
â”‚
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ actions/                      # Server Actions ('use server')
â”‚   â”‚   â”œâ”€â”€ analyze.ts                # Analyze prompt
â”‚   â”‚   â”œâ”€â”€ optimize.ts               # Optimize prompt
â”‚   â”‚   â”œâ”€â”€ preview.ts                # Generate preview
â”‚   â”‚   â”œâ”€â”€ compare.ts                # Multi-provider comparison
â”‚   â”‚   â”œâ”€â”€ health.ts                 # Provider health checks
â”‚   â”‚   â”œâ”€â”€ discover-models.ts        # Model discovery (Ollama)
â”‚   â”‚   â””â”€â”€ providers.ts              # Provider management
â”‚   â”‚
â”‚   â”œâ”€â”€ providers/                    # AI Provider Adapters
â”‚   â”‚   â”œâ”€â”€ types.ts                  # Shared types & interfaces
â”‚   â”‚   â”œâ”€â”€ base.ts                   # Base AIProvider interface
â”‚   â”‚   â”œâ”€â”€ factory.ts                # Provider Factory
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ commercial/               # Commercial providers
â”‚   â”‚   â”‚   â”œâ”€â”€ anthropic.ts          # Anthropic Claude
â”‚   â”‚   â”‚   â”œâ”€â”€ openai.ts             # OpenAI GPT
â”‚   â”‚   â”‚   â”œâ”€â”€ google.ts             # Google Gemini
â”‚   â”‚   â”‚   â””â”€â”€ cohere.ts             # Cohere (P3)
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ open-source/              # Open-source providers
â”‚   â”‚   â”‚   â”œâ”€â”€ ollama.ts             # Ollama (local)
â”‚   â”‚   â”‚   â”œâ”€â”€ huggingface.ts        # Hugging Face
â”‚   â”‚   â”‚   â”œâ”€â”€ together.ts           # Together AI (P2)
â”‚   â”‚   â”‚   â””â”€â”€ replicate.ts          # Replicate (P2)
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ adapters/                 # Generic adapters
â”‚   â”‚   â”‚   â”œâ”€â”€ openai-compatible.ts  # For Together AI, Ollama
â”‚   â”‚   â”‚   â””â”€â”€ base-adapter.ts
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ errors.ts                 # Custom error classes
â”‚   â”‚
â”‚   â”œâ”€â”€ cache/                        # Cache utilities for "use cache"
â”‚   â”‚   â”œâ”€â”€ helpers.ts                # Cache key generation
â”‚   â”‚   â””â”€â”€ config.ts                 # Cache profiles
â”‚   â”‚
â”‚   â”œâ”€â”€ prompts/                      # System prompts
â”‚   â”‚   â”œâ”€â”€ analysis.ts               # Prompts for analyzing
â”‚   â”‚   â”œâ”€â”€ optimization.ts           # Prompts for optimizing
â”‚   â”‚   â””â”€â”€ templates.ts              # Reusable templates
â”‚   â”‚
â”‚   â”œâ”€â”€ rate-limit/                   # Rate limiting
â”‚   â”‚   â””â”€â”€ config.ts                 # Rate limit configuration
â”‚   â”‚
â”‚   â”œâ”€â”€ utils.ts                      # Utility functions
â”‚   â””â”€â”€ constants.ts                  # App constants
â”‚
â”œâ”€â”€ types/
â”‚   â”œâ”€â”€ index.ts                      # Global TypeScript types
â”‚   â”œâ”€â”€ providers.ts                  # Provider-specific types
â”‚   â””â”€â”€ analysis.ts                   # Analysis result types
â”‚
â””â”€â”€ config/
    â”œâ”€â”€ providers.ts                  # Provider configurations
    â””â”€â”€ models.ts                     # Model configurations
```

---

## Provider Implementation Details

### 1. Ollama Provider (Local, Free)

**Key Features:**
- Runs completely locally (privacy)
- No API costs
- Dynamic model discovery
- Supports streaming
- OpenAI-compatible API

**Implementation:**

```typescript
// lib/providers/open-source/ollama.ts
import { Ollama } from 'ollama'
import type { AIProvider, ProviderMetadata } from '../types'

export class OllamaProvider implements AIProvider {
  private client: Ollama

  readonly metadata: ProviderMetadata = {
    name: 'ollama',
    displayName: 'Ollama (Local)',
    category: 'open-source',
    location: 'local',
    requiresApiKey: false,
    isOpenAICompatible: true,
    supportsModelDiscovery: true,
    icon: 'ğŸ¦™',
    documentation: 'https://ollama.ai',
    defaultEndpoint: 'http://127.0.0.1:11434',
  }

  readonly defaultModel = 'llama3.2'

  // Health check - verify Ollama is running
  async healthCheck(): Promise<HealthStatus> {
    try {
      const response = await this.client.list()
      return {
        available: true,
        latency: Date.now() - startTime,
        modelsCount: response.models?.length || 0,
      }
    } catch (error) {
      return {
        available: false,
        error: 'Ollama is not running. Please start Ollama.',
      }
    }
  }

  // Discover installed models dynamically
  async discoverModels(): Promise<Model[]> {
    const response = await this.client.list()
    return response.models.map(m => ({
      id: m.name,
      name: this.formatModelName(m.name),
      tier: 'free',
      provider: 'ollama',
      openSource: true,
    }))
  }

  // Implementation continues...
}
```

### 2. Anthropic Provider (Commercial)

**Key Features:**
- Claude 3.5 Sonnet, Opus, Haiku
- Best quality analysis
- Official SDK
- Streaming support

**Implementation:**

```typescript
// lib/providers/commercial/anthropic.ts
import Anthropic from '@anthropic-ai/sdk'
import type { AIProvider } from '../types'

export class AnthropicProvider implements AIProvider {
  private client: Anthropic

  readonly metadata = {
    name: 'anthropic',
    displayName: 'Anthropic Claude',
    category: 'commercial',
    location: 'cloud',
    requiresApiKey: true,
    icon: 'ğŸ¤–',
    // ...
  }

  async analyzePrompt(prompt: string, model?: string): Promise<AnalysisResult> {
    const response = await this.client.messages.create({
      model: model || this.defaultModel,
      max_tokens: 2000,
      messages: [{
        role: 'user',
        content: `Analyze this AI prompt for issues: "${prompt}"`
      }]
    })

    return this.parseAnalysis(response.content[0].text)
  }

  // Implementation continues...
}
```

### 3. Hugging Face Provider (Open-Source)

**Key Features:**
- 1000+ models available
- Free tier
- Cloud-based
- Official SDK

**Implementation:**

```typescript
// lib/providers/open-source/huggingface.ts
import { HfInference } from '@huggingface/inference'
import type { AIProvider } from '../types'

export class HuggingFaceProvider implements AIProvider {
  private client: HfInference

  readonly metadata = {
    name: 'huggingface',
    displayName: 'Hugging Face',
    category: 'open-source',
    location: 'cloud',
    requiresApiKey: true,
    icon: 'ğŸ¤—',
    // ...
  }

  readonly models = [
    {
      id: 'meta-llama/Llama-3.2-3B-Instruct',
      name: 'Llama 3.2 3B Instruct',
      tier: 'free',
      openSource: true,
    },
    // More models...
  ]

  // Implementation continues...
}
```

### 4. Together AI Provider (OpenAI-Compatible)

**Key Features:**
- 200+ open-source models
- Fast inference (up to 4x faster than vLLM)
- OpenAI-compatible API
- Can reuse OpenAI SDK

**Implementation:**

```typescript
// lib/providers/open-source/together.ts
import OpenAI from 'openai'
import type { AIProvider } from '../types'

export class TogetherProvider implements AIProvider {
  private client: OpenAI

  constructor(config: { apiKey: string }) {
    // Together AI is OpenAI-compatible!
    this.client = new OpenAI({
      apiKey: config.apiKey,
      baseURL: 'https://api.together.xyz/v1',
    })
  }

  readonly metadata = {
    name: 'together',
    displayName: 'Together AI',
    category: 'open-source',
    location: 'cloud',
    isOpenAICompatible: true,
    icon: 'âš¡',
    // ...
  }

  // Can use standard OpenAI SDK methods
  async analyzePrompt(prompt: string, model?: string) {
    const response = await this.client.chat.completions.create({
      model: model || this.defaultModel,
      messages: [{ role: 'user', content: `Analyze: ${prompt}` }]
    })
    return this.parseResponse(response)
  }
}
```

### Provider Factory Pattern

**Purpose:** Create and manage provider instances

```typescript
// lib/providers/factory.ts
export class ProviderFactory {
  private static instances = new Map<ProviderType, AIProvider>()

  static create(providerName: ProviderType): AIProvider {
    // Singleton pattern - reuse instances
    if (this.instances.has(providerName)) {
      return this.instances.get(providerName)!
    }

    let provider: AIProvider

    switch (providerName) {
      case 'anthropic':
        provider = new AnthropicProvider({ apiKey: process.env.ANTHROPIC_API_KEY! })
        break
      case 'ollama':
        provider = new OllamaProvider({ endpoint: process.env.OLLAMA_ENDPOINT })
        break
      // ... other providers
    }

    this.instances.set(providerName, provider)
    return provider
  }

  // Get all available providers with health status
  static async getAvailableProviders(): Promise<ProviderInfo[]> {
    const providers = ['anthropic', 'openai', 'google', 'ollama', 'huggingface']

    return Promise.all(
      providers.map(async (name) => {
        const provider = this.create(name)
        const health = await provider.healthCheck()
        return {
          ...provider.metadata,
          available: health.available,
          latency: health.latency,
        }
      })
    )
  }
}
```

---

## Next.js 16 Features Implementation

### 1. Turbopack (Stable)

**Default bundler in Next.js 16 - no configuration needed**

Benefits:
- 10x faster Fast Refresh
- 2-5x faster production builds
- Filesystem caching

### 2. "use cache" Directive

**Component-level caching:**

```typescript
// components/server/OptimizedPromptDisplay.tsx
async function OptimizedPromptDisplay({ prompt, provider }) {
  'use cache'  // Cache this component's output

  const optimized = await optimizePrompt(prompt, provider)

  return (
    <div>
      <h3>Optimized Version:</h3>
      <p>{optimized.text}</p>
    </div>
  )
}
```

**Function-level caching:**

```typescript
// lib/providers/anthropic.ts
export async function getAnthropicModels() {
  'use cache'  // Cache the model list

  return [
    { id: 'claude-3-5-sonnet-20241022', name: 'Claude 3.5 Sonnet' },
    { id: 'claude-3-opus-20240229', name: 'Claude 3 Opus' },
    { id: 'claude-3-haiku-20240307', name: 'Claude 3 Haiku' },
  ]
}
```

**Configuration:**

```typescript
// next.config.ts
const nextConfig: NextConfig = {
  cacheComponents: true,  // Enable Cache Components
}
```

### 3. proxy.ts (Replaces middleware.ts)

**Rate limiting implementation:**

```typescript
// proxy.ts (root level)
import { NextResponse, NextRequest } from 'next/server'
import { Ratelimit } from '@upstash/ratelimit'
import { Redis } from '@upstash/redis'

const ratelimit = new Ratelimit({
  redis: Redis.fromEnv(),
  limiter: Ratelimit.slidingWindow(20, '60 s'),
})

export async function proxy(request: NextRequest) {
  const ip = request.ip ?? 'anonymous'
  const { success, limit, remaining } = await ratelimit.limit(ip)

  if (!success) {
    return NextResponse.json(
      { error: 'Too many requests' },
      {
        status: 429,
        headers: { 'X-RateLimit-Remaining': remaining.toString() }
      }
    )
  }

  return NextResponse.next()
}

export const config = {
  matcher: ['/((?!_next/static|_next/image|favicon.ico).*)'],
}
```

### 4. Server Actions

**Type-safe API without routes:**

```typescript
// lib/actions/optimize.ts
'use server'

export async function optimizePrompt(
  prompt: string,
  providerName: string,
  model?: string
): Promise<OptimizationResult> {
  // Server-side only code
  const provider = ProviderFactory.create(providerName)
  const analysis = await provider.analyzePrompt(prompt, model)
  const optimized = await provider.optimizePrompt(prompt, analysis, model)

  return {
    original: prompt,
    optimized: optimized.text,
    improvements: optimized.improvements,
  }
}
```

**Usage in Client Component:**

```typescript
// components/client/PromptOptimizer.tsx
'use client'

import { optimizePrompt } from '@/lib/actions/optimize'

export function PromptOptimizer() {
  async function handleOptimize() {
    // Type-safe server action call
    const result = await optimizePrompt(prompt, provider)
    setResult(result)
  }

  return <button onClick={handleOptimize}>Optimize</button>
}
```

### 5. React 19.2 Features

**View Transitions:**

```typescript
// next.config.ts
const nextConfig: NextConfig = {
  reactOptions: {
    viewTransitions: true,  // Enable View Transitions API
  },
}
```

**React Compiler:**

```typescript
// next.config.ts
const nextConfig: NextConfig = {
  experimental: {
    reactCompiler: true,  // Automatic memoization
  },
}
```

### 6. Complete next.config.ts

```typescript
// next.config.ts
import type { NextConfig } from 'next'

const nextConfig: NextConfig = {
  // Enable Cache Components (Next.js 16)
  cacheComponents: true,

  // React 19.2 with View Transitions
  reactOptions: {
    viewTransitions: true,
  },

  // React Compiler (automatic memoization)
  experimental: {
    reactCompiler: true,
  },

  // TypeScript strict mode
  typescript: {
    ignoreBuildErrors: false,
  },

  // Environment variables
  env: {
    APP_NAME: 'EasyPrompt',
    APP_VERSION: '1.0.0',
  },
}

export default nextConfig
```

---

## Implementation Phases

### Phase 1: Foundation (Day 1 - Morning) â­

**Goal:** Setup Next.js 16 project with core dependencies

- [ ] Initialize Next.js 16 with TypeScript 5.1+
- [ ] Configure next.config.ts (cacheComponents, reactCompiler)
- [ ] Setup Turbopack (default)
- [ ] Install dependencies:
  - React 19.2, TypeScript 5.1+, Tailwind CSS 4
  - AI SDKs: `@anthropic-ai/sdk`, `openai`, `@google/generative-ai`
  - Open-source: `ollama`, `@huggingface/inference`, `replicate`
  - Rate limiting: `@upstash/ratelimit`, `@upstash/redis`
- [ ] Configure Tailwind CSS v4
- [ ] Setup TypeScript strict mode
- [ ] Create environment variables structure (.env.example)
- [ ] Create .gitignore

**Deliverable:** Working Next.js 16 project with all dependencies

### Phase 2: Provider Infrastructure (Day 1 - Afternoon)

**Goal:** Build provider abstraction layer

- [ ] Create provider types and interfaces (`lib/providers/types.ts`)
- [ ] Build base AIProvider interface (`lib/providers/base.ts`)
- [ ] Implement ProviderFactory with singleton pattern (`lib/providers/factory.ts`)
- [ ] Create commercial provider adapters:
  - [ ] Anthropic Claude (`lib/providers/commercial/anthropic.ts`)
  - [ ] OpenAI GPT (`lib/providers/commercial/openai.ts`)
  - [ ] Google Gemini (`lib/providers/commercial/google.ts`)
- [ ] Create open-source provider adapters:
  - [ ] Ollama (`lib/providers/open-source/ollama.ts`)
  - [ ] Hugging Face (`lib/providers/open-source/huggingface.ts`)
- [ ] Add provider error handling (`lib/providers/errors.ts`)
- [ ] Test provider instantiation

**Deliverable:** Working provider abstraction with 5 providers

### Phase 3: Server Actions (Day 1 - Evening)

**Goal:** Create type-safe server actions

- [ ] Create `lib/actions/analyze.ts` - Analyze prompt
- [ ] Create `lib/actions/optimize.ts` - Optimize prompt
- [ ] Create `lib/actions/preview.ts` - Generate preview
- [ ] Create `lib/actions/compare.ts` - Multi-provider comparison
- [ ] Create `lib/actions/health.ts` - Provider health checks
- [ ] Create `lib/actions/discover-models.ts` - Ollama model discovery
- [ ] Create `lib/actions/providers.ts` - Provider management
- [ ] Add input validation for all actions
- [ ] Add error handling with try-catch
- [ ] Implement response normalization

**Deliverable:** Working server actions with type safety

### Phase 4: Rate Limiting & Security (Day 2 - Morning)

**Goal:** Implement security measures

- [ ] Create `proxy.ts` file for rate limiting
- [ ] Configure Upstash Redis connection (or in-memory for dev)
- [ ] Implement per-IP rate limiting (20 requests/minute)
- [ ] Add rate limit headers to responses
- [ ] Test rate limiting functionality
- [ ] Add API key validation on startup
- [ ] Add input sanitization
- [ ] Create custom error pages (429, 500, etc.)

**Deliverable:** Production-ready rate limiting

### Phase 5: UI Components - Client Side (Day 2 - Afternoon)

**Goal:** Build interactive client components

- [ ] Create `components/client/PromptInput.tsx` - Text area
- [ ] Create `components/client/ProviderSelector.tsx` - Dropdown with categories
- [ ] Create `components/client/ModelSelector.tsx` - Model selection
- [ ] Create `components/client/ProviderHealthCheck.tsx` - Real-time health
- [ ] Create `components/client/LocalProviderSetup.tsx` - Setup instructions
- [ ] Create `components/client/OptimizeButton.tsx` - Submit button
- [ ] Create `components/client/CopyButton.tsx` - Copy to clipboard
- [ ] Add loading states and animations
- [ ] Add error display components

**Deliverable:** Interactive UI components

### Phase 6: UI Components - Server Side (Day 2 - Evening)

**Goal:** Build server components with caching

- [ ] Create `components/server/OptimizedPromptDisplay.tsx` with "use cache"
- [ ] Create `components/server/AnalysisResults.tsx`
- [ ] Create `components/server/ComparisonGrid.tsx`
- [ ] Create `components/server/ProviderBadge.tsx`
- [ ] Create `components/server/ProviderHealthStatus.tsx`
- [ ] Create `components/server/ModelList.tsx`
- [ ] Create `components/server/ImprovementsList.tsx`
- [ ] Add `app/loading.tsx` for loading states
- [ ] Add `app/error.tsx` for error boundaries
- [ ] Implement Suspense boundaries

**Deliverable:** Server components with caching

### Phase 7: Home Page Integration (Day 3 - Morning)

**Goal:** Build main optimizer page

- [ ] Create `app/page.tsx` (Server Component)
- [ ] Integrate PromptOptimizer component
- [ ] Add responsive layout (mobile-first)
- [ ] Implement Tailwind animations
- [ ] Add hero section with description
- [ ] Test end-to-end optimization flow
- [ ] Test with multiple providers
- [ ] Add keyboard shortcuts (optional)

**Deliverable:** Working home page

### Phase 8: Multi-Provider Comparison (Day 3 - Afternoon)

**Goal:** Build comparison feature

- [ ] Create `app/compare/page.tsx`
- [ ] Build `components/client/MultiProviderComparison.tsx`
- [ ] Implement parallel provider calls
- [ ] Show side-by-side results grid
- [ ] Add provider performance indicators (speed, cost)
- [ ] Add filter by category (commercial/open-source)
- [ ] Add export comparison as JSON/CSV

**Deliverable:** Working comparison page

### Phase 9: Provider Management (Day 3 - Evening)

**Goal:** Build provider status page

- [ ] Create `app/providers/page.tsx`
- [ ] Show all providers with health status
- [ ] Display available models per provider
- [ ] Add Ollama model discovery with refresh button
- [ ] Show setup instructions for each provider
- [ ] Add provider documentation links
- [ ] Show cost estimates per provider
- [ ] Add provider troubleshooting guide

**Deliverable:** Provider management page

### Phase 10: Templates & Guide (Day 4 - Morning)

**Goal:** Add educational content

- [ ] Create `app/templates/page.tsx`
- [ ] Add 15+ prompt templates:
  - Writing (blog posts, emails, stories)
  - Coding (refactoring, debugging, documentation)
  - Analysis (data, research, summarization)
  - Creative (brainstorming, marketing, design)
  - Education (explanations, learning plans)
- [ ] Create `app/guide/page.tsx`
- [ ] Add best practices guide
- [ ] Add provider-specific tips
- [ ] Add local vs cloud comparison guide
- [ ] Implement template copy functionality
- [ ] Add template search/filter

**Deliverable:** Educational content

### Phase 11: Polish & Optimization (Day 4 - Afternoon)

**Goal:** Production-ready polish

- [ ] Apply "use cache" to all cacheable components
- [ ] Add React 19.2 View Transitions
- [ ] Optimize images with Next.js Image
- [ ] Optimize bundle size (analyze with next/bundle-analyzer)
- [ ] Add meta tags for SEO
- [ ] Add Open Graph tags for social sharing
- [ ] Test on mobile devices (responsive design)
- [ ] Run Lighthouse audit (target: 95+ all metrics)
- [ ] Add error boundary testing
- [ ] Add accessibility testing (WCAG 2.1 AA)
- [ ] Add analytics (optional)

**Deliverable:** Polished, production-ready app

### Phase 12: Advanced Features (Optional - Day 4 - Evening)

**Goal:** Enhanced functionality

- [ ] Implement Together AI adapter (P2)
- [ ] Implement Replicate adapter (P2)
- [ ] Add cost estimation per provider
- [ ] Add provider usage analytics
- [ ] Save optimization history (localStorage)
- [ ] Share optimization via URL (encode in query params)
- [ ] Add dark mode toggle
- [ ] Add prompt history (last 10 prompts)
- [ ] Add export results as PDF/Markdown

**Deliverable:** Advanced features

### Phase 13: Deployment (Day 5)

**Goal:** Deploy to production

- [ ] Setup Vercel project
- [ ] Configure all environment variables in Vercel
- [ ] Setup Upstash Redis (production)
- [ ] Test with production builds locally
- [ ] Deploy to Vercel
- [ ] Test live deployment
- [ ] Setup custom domain (optional)
- [ ] Configure DNS
- [ ] Setup error tracking (Sentry - optional)
- [ ] Monitor performance and errors
- [ ] Write README documentation
- [ ] Create user documentation
- [ ] Create contributing guide (if open-source)

**Deliverable:** Live production deployment

---

## Environment Configuration

### Environment Variables

```env
# .env.local (NEVER commit this file)
# Copy from .env.example and fill in your values

# Node.js version (required: 20.9.0+)
NODE_VERSION=20.9.0

# ========================================
# Commercial AI Providers
# ========================================

# Anthropic Claude (get from: https://console.anthropic.com)
ANTHROPIC_API_KEY=sk-ant-api03-...

# OpenAI (get from: https://platform.openai.com/api-keys)
OPENAI_API_KEY=sk-...

# Google Gemini (get from: https://aistudio.google.com/app/apikey)
GOOGLE_API_KEY=...

# ========================================
# Open-Source AI Providers
# ========================================

# Ollama (Local) - Optional custom endpoint
# Default: http://127.0.0.1:11434
OLLAMA_ENDPOINT=http://127.0.0.1:11434

# Hugging Face (get from: https://huggingface.co/settings/tokens)
# Free tier available!
HUGGINGFACE_API_KEY=hf_...

# Together AI (get from: https://api.together.xyz)
# Optional - Phase 2
TOGETHER_API_KEY=...

# Replicate (get from: https://replicate.com/account/api-tokens)
# Optional - Phase 2
REPLICATE_API_KEY=r8_...

# Cohere (get from: https://dashboard.cohere.com/api-keys)
# Optional - Phase 3
COHERE_API_KEY=...

# ========================================
# Rate Limiting (Upstash Redis)
# ========================================

# Get from: https://console.upstash.com/
# Free tier: 10,000 commands/day
UPSTASH_REDIS_REST_URL=https://...
UPSTASH_REDIS_REST_TOKEN=...

# Alternative: Use in-memory for development
# Set to "true" to disable Upstash and use in-memory
USE_MEMORY_RATE_LIMIT=false

# ========================================
# App Configuration
# ========================================

NEXT_PUBLIC_APP_NAME=EasyPrompt
NEXT_PUBLIC_APP_URL=https://easyprompt.com
NEXT_PUBLIC_MAX_PROMPT_LENGTH=5000
NEXT_PUBLIC_ENABLE_LOCAL_PROVIDERS=true

# ========================================
# Feature Flags
# ========================================

# Enable/disable specific providers
ENABLE_ANTHROPIC=true
ENABLE_OPENAI=true
ENABLE_GOOGLE=true
ENABLE_OLLAMA=true
ENABLE_HUGGINGFACE=true
ENABLE_TOGETHER=false  # Phase 2
ENABLE_REPLICATE=false # Phase 2
ENABLE_COHERE=false    # Phase 3

# ========================================
# Optional: Analytics
# ========================================

# Google Analytics (optional)
NEXT_PUBLIC_GA_ID=G-...

# Posthog (optional)
NEXT_PUBLIC_POSTHOG_KEY=...
NEXT_PUBLIC_POSTHOG_HOST=https://app.posthog.com

# ========================================
# Development
# ========================================

# Log level: debug, info, warn, error
LOG_LEVEL=info

# Enable debug mode
DEBUG=false
```

### .env.example Template

Create `.env.example` (commit this) with placeholder values:

```env
# Copy this to .env.local and fill in your values

# Commercial AI Providers
ANTHROPIC_API_KEY=your_anthropic_api_key_here
OPENAI_API_KEY=your_openai_api_key_here
GOOGLE_API_KEY=your_google_api_key_here

# Open-Source AI Providers
OLLAMA_ENDPOINT=http://127.0.0.1:11434
HUGGINGFACE_API_KEY=your_huggingface_api_key_here
TOGETHER_API_KEY=your_together_api_key_here
REPLICATE_API_KEY=your_replicate_api_key_here

# Rate Limiting
UPSTASH_REDIS_REST_URL=your_upstash_redis_url
UPSTASH_REDIS_REST_TOKEN=your_upstash_redis_token
USE_MEMORY_RATE_LIMIT=true

# Feature Flags
ENABLE_ANTHROPIC=true
ENABLE_OPENAI=true
ENABLE_GOOGLE=true
ENABLE_OLLAMA=true
ENABLE_HUGGINGFACE=true
```

---

## Dependencies

### package.json

```json
{
  "name": "easyprompt",
  "version": "1.0.0",
  "description": "AI prompt optimization platform with multi-provider support",
  "private": true,
  "engines": {
    "node": ">=20.9.0",
    "npm": ">=10.0.0"
  },
  "scripts": {
    "dev": "next dev --turbopack",
    "build": "next build",
    "start": "next start",
    "lint": "next lint",
    "lint:fix": "next lint --fix",
    "type-check": "tsc --noEmit",
    "format": "prettier --write \"**/*.{ts,tsx,md,json}\"",
    "test": "vitest",
    "analyze": "ANALYZE=true next build"
  },
  "dependencies": {
    "next": "^16.0.0",
    "react": "^19.2.0",
    "react-dom": "^19.2.0",

    "@anthropic-ai/sdk": "^0.30.0",
    "openai": "^4.70.0",
    "@google/generative-ai": "^0.21.0",

    "ollama": "^0.6.3",
    "@huggingface/inference": "^4.13.3",
    "replicate": "^0.34.1",

    "@upstash/ratelimit": "^2.0.0",
    "@upstash/redis": "^1.34.0",

    "tailwindcss": "^4.0.0",
    "class-variance-authority": "^0.7.0",
    "clsx": "^2.1.0",
    "tailwind-merge": "^2.5.0",

    "@radix-ui/react-select": "^2.0.0",
    "@radix-ui/react-tabs": "^1.0.4",
    "@radix-ui/react-dialog": "^1.0.5",
    "@radix-ui/react-dropdown-menu": "^2.0.6",
    "lucide-react": "^0.344.0",

    "zod": "^3.22.4"
  },
  "devDependencies": {
    "@types/node": "^20.9.0",
    "@types/react": "^19.0.0",
    "@types/react-dom": "^19.0.0",
    "typescript": "^5.1.0",
    "eslint": "^9.0.0",
    "eslint-config-next": "^16.0.0",
    "prettier": "^3.0.0",
    "prettier-plugin-tailwindcss": "^0.5.0",
    "@next/bundle-analyzer": "^16.0.0",
    "vitest": "^1.0.0"
  }
}
```

---

## Success Metrics

### Performance Targets

| Metric | Target | Measurement Tool |
|--------|--------|------------------|
| Page Load Time | < 1.5s | Lighthouse, Web Vitals |
| Optimization Time | < 3s (avg) | Performance API |
| Mobile Performance | 95+ | Lighthouse Mobile |
| Desktop Performance | 95+ | Lighthouse Desktop |
| Accessibility | 100 | Lighthouse A11y |
| SEO | 100 | Lighthouse SEO |
| Best Practices | 95+ | Lighthouse |
| Bundle Size (JS) | < 250KB | Next.js build output |
| Server Action Response | < 2s (p95) | Monitoring |
| Provider Health Check | < 500ms | API monitoring |
| Ollama Model Discovery | < 1s | Local API test |

### Feature Completeness

| Feature | P1 (MVP) | P2 | P3 |
|---------|----------|----|----|
| Anthropic Claude | âœ… | | |
| OpenAI GPT | âœ… | | |
| Google Gemini | âœ… | | |
| Ollama (Local) | âœ… | | |
| Hugging Face | âœ… | | |
| Together AI | | âœ… | |
| Replicate | | âœ… | |
| Cohere | | | âœ… |
| Rate Limiting | âœ… | | |
| Multi-Provider Comparison | âœ… | | |
| Templates Library | âœ… | | |
| Best Practices Guide | âœ… | | |
| Provider Health Monitoring | âœ… | | |
| Model Discovery (Ollama) | âœ… | | |
| Dark Mode | | âœ… | |
| Prompt History | | âœ… | |
| Cost Estimation | | âœ… | |
| Share via URL | | âœ… | |

### Quality Metrics

- **Type Coverage:** 100% (strict TypeScript)
- **Test Coverage:** 80%+ (if implementing tests)
- **Accessibility:** WCAG 2.1 AA compliance
- **Browser Support:** Last 2 versions of major browsers
- **Mobile Support:** iOS Safari, Chrome, Android Chrome

---

## Deployment Guide

### Pre-Deployment Checklist

- [ ] All environment variables configured in Vercel
- [ ] Upstash Redis setup complete
- [ ] At least one AI provider API key configured
- [ ] Rate limiting tested
- [ ] Production build successful locally
- [ ] TypeScript compilation successful
- [ ] ESLint passes
- [ ] Lighthouse audit > 95
- [ ] Mobile testing complete
- [ ] Error boundaries tested
- [ ] 404/500 pages customized

### Vercel Deployment Steps

1. **Create Vercel Project**
   ```bash
   npm i -g vercel
   vercel login
   vercel
   ```

2. **Configure Environment Variables**
   - Go to Vercel Dashboard â†’ Project â†’ Settings â†’ Environment Variables
   - Add all variables from `.env.local`
   - Set for Production, Preview, and Development

3. **Configure Build Settings**
   - Build Command: `next build`
   - Output Directory: `.next`
   - Install Command: `npm install`
   - Node Version: 20.x

4. **Deploy**
   ```bash
   vercel --prod
   ```

5. **Custom Domain (Optional)**
   - Go to Vercel Dashboard â†’ Project â†’ Settings â†’ Domains
   - Add custom domain
   - Update DNS records (CNAME or A record)
   - Wait for SSL certificate

### Post-Deployment

- [ ] Test all providers on production
- [ ] Monitor error rates (Vercel Analytics or Sentry)
- [ ] Test rate limiting
- [ ] Verify environment variables
- [ ] Test Ollama connection (if using)
- [ ] Check performance (Lighthouse on live URL)
- [ ] Setup monitoring alerts

### Monitoring

**Recommended Tools:**
- **Vercel Analytics** - Built-in performance monitoring
- **Sentry** - Error tracking
- **Upstash Console** - Rate limit monitoring
- **Google Analytics** - User behavior (optional)

---

## Additional Documentation

### README.md Structure

```markdown
# EasyPrompt

AI Prompt Optimization Platform with Multi-Provider Support

## Features
- Multiple AI providers (Anthropic, OpenAI, Google, Ollama, Hugging Face)
- Local model support via Ollama
- Real-time optimization
- Before/after comparison
- Educational content

## Getting Started

### Prerequisites
- Node.js 20.9.0+
- At least one AI provider API key OR Ollama installed

### Installation
1. Clone the repository
2. Install dependencies: `npm install`
3. Copy `.env.example` to `.env.local`
4. Add your API keys
5. Run development server: `npm run dev`

### Using Ollama (Free, Local)
1. Install Ollama: https://ollama.ai
2. Pull a model: `ollama pull llama3.2`
3. Start EasyPrompt and select Ollama provider

## Tech Stack
- Next.js 16
- React 19.2
- TypeScript 5.1+
- Tailwind CSS 4

## License
MIT
```

### Contributing Guide (if open-source)

```markdown
# Contributing to EasyPrompt

## Adding a New Provider

1. Create provider file in `lib/providers/open-source/` or `lib/providers/commercial/`
2. Implement `AIProvider` interface
3. Add to `ProviderFactory`
4. Add environment variable
5. Update documentation
6. Test thoroughly

## Code Style
- TypeScript strict mode
- ESLint + Prettier
- Conventional commits

## Testing
- Run `npm run type-check`
- Run `npm run lint`
- Test all providers manually
```

---

## Conclusion

This architecture document provides a complete blueprint for building **EasyPrompt**, a production-ready AI prompt optimization platform with comprehensive multi-provider support including both commercial and open-source options.

**Key Highlights:**
- âœ… Next.js 16 with latest features
- âœ… 7 AI providers supported
- âœ… Local model support (Ollama)
- âœ… Type-safe architecture
- âœ… Production-ready security
- âœ… Comprehensive documentation

**Timeline:** 4-5 days for full MVP implementation

**Next Steps:** Begin Phase 1 implementation

---

**Document Version:** 1.0.0
**Last Updated:** 2025-11-18
**Status:** Ready for Implementation
